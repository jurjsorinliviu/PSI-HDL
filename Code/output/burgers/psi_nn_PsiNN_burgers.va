// Verilog-A Neural Network Module
// Auto-generated by Psi-HDL Framework
// Model Type: PsiNN_burgers
// Input Dimension: 2
// Output Dimension: 1
// Total Layers: 4
//
// This module implements a neural network discovered by Psi-NN
// as an analog behavioral model suitable for SPICE simulation.

`include "constants.vams"
`include "disciplines.vams"


module psi_nn_PsiNN_burgers(in0, in1, out0, vdd, vss);
    // Input ports (voltage nodes)
    input in0;
    input in1;

    // Output ports (voltage nodes)
    output out0;

    // Power supply nodes
    input vdd, vss;
    
    // Declare port types as electrical
    electrical in0;
    electrical in1;
    electrical out0;
    electrical vdd, vss;


    // Neural network parameters (weights and biases)
    // These can be overridden during instantiation

    // Layer 0: layer1 (psi_plus_minus)
    parameter real fc1_1_w[0:19][0:0] = 0.0;
    parameter real fc1_1_b[0:19] = 0.0;
    parameter real fc1_3_w[0:19][0:0] = 0.0;

    // Layer 1: layer2 (psi_symmetric)
    parameter real fc2_1_w[0:19][0:19] = 0.0;
    parameter real fc2_2_w[0:19][0:19] = 0.0;
    parameter real fc2_3_w[0:19][0:19] = 0.0;

    // Layer 2: layer3 (psi_combination)
    parameter real fc3_1_w[0:39][0:19] = 0.0;
    parameter real fc3_2_w[0:39][0:19] = 0.0;

    // Layer 3: output (output)
    parameter real output_w[0:0][0:39] = 0.0;


    // Internal variables for layer computations
    real input_vec[0:1];
    real output_vec[0:0];
    real layer_out[0:59];
    real temp_sum;
    integer i, j;


    analog begin
        // Read input voltages and normalize
        input_vec[0] = V(in0) / 1.0;
        input_vec[1] = V(in1) / 1.0;

        // Forward propagation through network

        // Layer 1: +/- structure with tanh activation
        // u1_1 = tanh(fc1_1(x) + fc1_3(y))
        // u1_2 = tanh(fc1_1(x) - fc1_3(y))
        
        // Compute fc1_1(x) contribution
        temp_sum = 0.0;
        for (i = 0; i < 1; i = i + 1) begin
            temp_sum = fc1_1_w[0][i] * input_vec[0] + fc1_1_b[0];
        end
        
        // Branch 1: + fc1_3(y)
        layer_out[0] = tanh(temp_sum + fc1_3_w[0][0] * input_vec[1]);
        
        // Branch 2: - fc1_3(y)
        layer_out[1] = tanh(temp_sum - fc1_3_w[0][0] * input_vec[1]);
        
        // Layer 2: Symmetric weight sharing
        // u2_1 = tanh(fc2_1(u1_1) + fc2_3(u1_2))
        // u2_2 = tanh(fc2_3(u1_1) + fc2_1(u1_2))
        // u2_3 = tanh(fc2_2(u1_1) - fc2_2(u1_2))
        
        // Store previous layer output
        real u1_1, u1_2;
        u1_1 = layer_out[0];
        u1_2 = layer_out[1];
        
        // Branch 1: fc2_1(u1_1) + fc2_3(u1_2)
        layer_out[0] = tanh(fc2_1_w[0][0] * u1_1 + fc2_1_b[0] + 
                           fc2_3_w[0][0] * u1_2 + fc2_3_b[0]);
        
        // Branch 2: fc2_3(u1_1) + fc2_1(u1_2)
        layer_out[1] = tanh(fc2_3_w[0][0] * u1_1 + fc2_3_b[0] + 
                           fc2_1_w[0][0] * u1_2 + fc2_1_b[0]);
        
        // Branch 3: fc2_2(u1_1) - fc2_2(u1_2)
        layer_out[2] = tanh(fc2_2_w[0][0] * (u1_1 - u1_2));
        
        // Layer 3: Combination
        // u3_1 = tanh(fc3_1(u2_1) - fc3_1(u2_2) + fc3_2(u2_3))
        real u2_1, u2_2, u2_3;
        u2_1 = layer_out[0];
        u2_2 = layer_out[1];
        u2_3 = layer_out[2];
        
        layer_out[0] = tanh(fc3_1_w[0][0] * u2_1 + fc3_1_b[0] - 
                           (fc3_1_w[0][0] * u2_2 + fc3_1_b[0]) +
                           fc3_2_w[0][0] * u2_3);
        
        // Layer 4: Output (linear, no bias for odd function)
        output_vec[0] = output_w[0][0] * layer_out[0];

        // Write output voltages
        V(out0) <+ output_vec[0] * 1.0;
    end


endmodule
